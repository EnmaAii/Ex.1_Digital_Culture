# -*- coding: utf-8 -*-
"""Копия блокнота "ЛР1 задание"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Y5IOi1aCy4aK6b98512fx-f8S4s2M1a

# Лабораторная работа 1
### "Классификация новостей"

В данной работе вам потребуется создать НС, которая определяет категорию новости (в тренировочном наборе данных их 4: спорт, IT, бизнес и преступления).

Для выполнения работы воспользуйтесь примером с практического занятия (распознавание языка) и набором данных, который прикреплён на курсе. Количество слоёв и все параметры нейросети устанавливаются на усмотрение студента (при желании студента можно воспользоваться любой архитектурой). Вывод статистики во время обучения не обязателен. Для создания словаря и приведения слов к числовому виду использовать код, который будет представлен ниже.

Также обратите внимание на класс ```Dataset``` в примере с занятия, а именно на его метод ```__getitem__```:

```python
    def __getitem__(self, idx): # data[0]
        text = vocab(list(self.data.iloc[idx]))
        return torch.tensor(text), torch.tensor(self.y.iloc[idx], dtype=torch.int64)

```
В примере при создании переменной ```text``` мы создавали из текста список (```list```), разбивая текст из датасета на **символы**. В данной лабораторной работе текст стоит разбивать на **слова**, для чего вместо ```list``` надо использовать ```tokenizer```, описанный ниже в коде для создания словаря.
```tokenizer``` - встроенная в библиотеку torchtext функция для преобразования текста.

Минимальная точность на тестовом наборе данных для получения зачёта по данной работе равна 87 процентам.

Контрольные вопросы (приблизительно такие же вопросы будут на защите этой работы):


1.   Чему равно количество входных и выходных слоёв нейросети для решения этой задачи в общем случае?
2.   Что такое learning rate?
3.   Как называется слой нейросети, в котором каждый нейрон получает значения от всех нейронов предыдущего слоя?
4.   Может ли быть в нейросети один выходной нейрон в задачах классификации?
5.   Что такое метрика?
6.   В чём отличие accuracy и log loss?
7.   В каком формате передаются слова на вход вашей нейросети?
9.   Какой алгоритм оптимизации весов Вы использовали при обучении нейросети?
10.   Приведите примеры другх задач, помимо классификации, для решения которых используются нейросети.

1.   **Загрузка датасета и присвоение номера категории**
"""

from google.colab import files
files.upload()

import pandas as pd
data = pd.read_csv("ag_news.csv", sep=';')
data.head()

"""

2.   **Обработка данных**




"""

data.columns[data.isna().any()].tolist() #проверить наличие NaN в DataFrame, если массив пуст, то и пропущенных данных нет

data["category"] = data["category"].astype('category') # вместо хранения каждого уникального текстового значения, pandas будет хранить уникальные категории и присваивать каждой категории числовой код.
data["category"] = data["category"].cat.codes # извлекаем числовые коды, присвоенные каждой переменной
data.head()

category_count = data["category"].max() + 1
category_count

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

tokenizer = get_tokenizer('basic_english')
# get_tokenizer - функция для обработки текста
# аргумент "basic_english" - это формат обработки (приведение к нижнему регистру, избавление от спецсимволов, ...)
# этот же аргумент можно указывать при обработке текстов на других языках!

def yield_tokens(data_train):
  for text in data_train:
    yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(data["content"]), specials=["<unk>"]) # здесь data_train - тренировочные данные
vocab.set_default_index(vocab["<unk>"])  # какой индекс нужно вернуть, если слово не найдено в словаре

vocab(['feel', 'i', 'say', 'hello', 'hi']) # проверка

"""

3.   **Создание классов для загрузки данных и разделение данных на тренировочные / тестировочные**


"""

from torch.utils.data import Dataset # для работы с датасетом
import torch # для НС и тд

class TextDataset(Dataset):
    def __init__(self, data_text, y): # конструктор класса
        self.data = data_text # сам текст
        self.y = y # язык

    def __len__(self):   # len(trainset)
        return len(self.data)

    def __getitem__(self, idx): # data[0]
      text = vocab(tokenizer(self.data.iloc[idx]))
      return torch.tensor(text), torch.tensor(self.y.iloc[idx], dtype=torch.int64)

y = data["category"]
data = data['content']

from sklearn.model_selection import train_test_split
data_train, data_test, \
  y_train, y_test = train_test_split(data, y, test_size=0.3)

trainset = TextDataset(data_text=data_train, y=y_train)
testset = TextDataset(data_text=data_test, y=y_test)

def collate_batch(batch):
    label_list, text_list, offsets = [], [], [0]
    for (_text, _label) in batch:
         label_list.append(_label)
         processed_text = torch.tensor(_text, dtype=torch.int64)
         text_list.append(processed_text)
         offsets.append(processed_text.size(0))
    label_list = torch.tensor(label_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return label_list, text_list, offsets

# Загрузчики в НС
from torch.utils.data import DataLoader
trainloader = DataLoader(trainset, batch_size=8, shuffle=False,
                         collate_fn=collate_batch)

testloader = DataLoader(testset, batch_size=8, shuffle=False,
                        collate_fn=collate_batch)

num_class = len(set(y_train))
vocab_size = len(vocab)
print("Кол-во классов: ", num_class)
print("Размер словаря: ", vocab_size)

"""


4.    **Создание НС**

"""

from torch import nn

class TextClassificationModel(nn.Module):
    def __init__(self):
        super(TextClassificationModel, self).__init__() # вытаскивает методы из nn.Module
        self.bag = nn.EmbeddingBag(vocab_size, 64, sparse=True)  # слой-матрица
        # EmbeddingBag - для восприятия слов в виде двоичных векторов
        self.lin = nn.Linear(64, 4)  # на вход подаётся 64 числа - выход от self.bag, слой выдаёт 4 числа - вероятность каждой новости, их 4 типа
    def forward(self, text, offsets): # проход через НС
        return self.lin( self.bag(text, offsets) )

net = TextClassificationModel()

import torch.optim as optim
# В качестве loss function используем кросс-энтропию
criterion = nn.CrossEntropyLoss()

# В качестве оптимизатора - стохастический градиентный спуск
optimizer = optim.SGD(net.parameters(), lr=0.1)

"""


5.   **Обучение НС**

"""

losses = []
running_corrects = 0
net.train(True)
for epoch in range(70):
    running_loss = 0.0
    running_corrects = 0.0
    for i, data in enumerate(trainloader, 0):
        labels, inputs, offsets = data

        optimizer.zero_grad() # включаю оптимизатор
        outputs = net(inputs, offsets) # проход данных через НС
        _, preds = torch.max(outputs.data, 1) # поиск ответа
        loss = criterion(outputs, labels) # считаю log loss
        loss.backward()
        optimizer.step() #меняю веса

        running_loss += loss.item()
        # считаю accuracy:
        running_corrects += int(torch.sum(preds == labels.data)) / len(labels)
        if i % 10000 == 9999:
          print('[%d, %5d] loss: %.3f accuracy: %.3f' % (epoch + 1, i + 1, running_loss/10000, running_corrects/10000 ))
          losses += [running_loss/10000]
          running_loss = 0.0
          running_corrects = 0.0

print('Finished Training')

outputs

preds

"""

6. **Тестирование**

"""

net.train(False)
runninig_correct = 0
num_of_tests = 0
for data in testloader:
    labels, inputs, offsets = data

    output = net(inputs, offsets)
    _, predicted = torch.max(output, 1)

    runninig_correct += int(torch.sum(predicted == labels)) / len(labels)
    num_of_tests += 1

print(runninig_correct / num_of_tests)